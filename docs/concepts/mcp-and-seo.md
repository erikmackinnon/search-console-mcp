---
title: "MCP and SEO"
description: "How the Model Context Protocol changes SEO analysis."
---

Traditional SEO tools are built for humans: dashboards, charts, and spreadsheets. **Model Context Protocol (MCP)** tools are built for agents.

## The Semantic Gap

In traditional analysis:
1.  A human looks at a chart.
2.  The human notices a trend.
3.  The human looks for an explanation.

In Agentic SEO:
1.  The agent asks for a specific insight (e.g., "detect anomalies").
2.  The MCP server runs a mathematical check (Z-score, standard deviation).
3.  The server returns a structured report.
4.  The agent reasons about the *why* based on the *what*.

## Why it Matters

### 1. Handling Scale
A human can't manually check for keyword cannibalization across 10,000 pages every day. An agent with a `detect_cannibalization` tool can do it in seconds.

### 2. Reducing Hallucinations
When an LLM performs math, it's prone to errors. By offloading calculation to the MCP server (written in TypeScript), we ensure the agent is reasoning with 100% accurate metrics.

### 3. State-of-the-Art Intelligence
Our MCP server doesn't just pass strings; it implements SEO primitives like:
*   **Ranking Buckets:** Categorizing results into 'Top 3', 'Page 1', etc.
*   **Traffic Deltas:** Calculating whether a change is statistically significant.
*   **Opportunity Scoring:** Estimating potential clicks based on impression volume.

## The Future of SEO
We see a future where SEO is not about "reporting" but about "resolution." Instead of spending 4 hours on a monthly deck, you'll spend 5 minutes reviewing a summary from your SEO agent, which has already performed the deep-dive analysis using these tools.
